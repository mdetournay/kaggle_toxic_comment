{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CleanStringSpace(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,file_paths,y=None,file=False):\n",
    "        \n",
    "        clean_strings=[]\n",
    "        \n",
    "        for path in file_paths:\n",
    "            if file:\n",
    "                f=open(path,encoding=\"utf8\")\n",
    "                s=f.read()\n",
    "                f.close()\n",
    "            else:\n",
    "                s=path\n",
    "            s = s.replace(u'\\n', u' ')\n",
    "            s = s.replace(u'’', u' ')\n",
    "            s = s.replace(u\"'\", u' ')\n",
    "            s = ''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn'))\n",
    "            s = s.replace('.', ' ')\n",
    "            s = ''.join(e for e in s if (e.isalnum() or e == \" \")).lower()\n",
    "            while '  ' in s:\n",
    "                s = s.replace('  ', ' ')\n",
    "            clean_strings.append(s)\n",
    "            \n",
    "        return clean_strings\n",
    "    \n",
    "    def fit_transform(self,file_paths,y=None):\n",
    "        \n",
    "        return self.transform(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TokenizeDocs(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,string_list,y=None):\n",
    "        tokens_list=[x.split() for x in string_list]\n",
    "        return tokens_list\n",
    "    \n",
    "    def fit_transform(self,string_list,y=None):\n",
    "        return self.transform(string_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tagged_Document_Gen(file_list=None,con=None,chunk=1000,content='content'):\n",
    "    \n",
    "    if con is not None:\n",
    "        cleaner=CleanStringSpace()\n",
    "        tokenizer=TokenizeDocs()\n",
    "        \n",
    "        idx=0\n",
    "        file_list=np.array(file_list)\n",
    "        chunks=int(math.ceil(float(len(file_list))/float(chunk))/2)\n",
    "        for i in range(0,chunks):\n",
    "            text_chunk=file_list[i*chunk:(i+1)*chunk]\n",
    "            text_chunk=[\"'\"+str(x)+\"'\" for x in text_chunk]\n",
    "            text_chunk=','.join(text_chunk)\n",
    "            doc_texts= pd.read_sql(\"\"\"SELECT text FROM plaintext WHERE bucket='epo-history-documents' AND filepath IN(%s)\"\"\"%text_chunk,con=con)['text'].tolist()\n",
    "            \n",
    "            for text in doc_texts:\n",
    "                text=cleaner.transform(file_paths=[text],file=False)[0]\n",
    "                tokens=tokenizer.transform([text])[0]\n",
    "                doc=doc2vec.TaggedDocument(tokens,[idx])\n",
    "                idx+=1\n",
    "                yield doc\n",
    "                \n",
    "            \n",
    "    elif content=='filename':\n",
    "        cleaner=CleanStringSpace()\n",
    "        tokenizer=TokenizeDocs()\n",
    "        for idx,row in file_list.iterrows():\n",
    "        \n",
    "            text=cleaner.transform(file_paths=[row[0]])[0]\n",
    "            tokens=tokenizer.transform([text])[0]\n",
    "            doc=gensim.models.doc2vec.TaggedDocument(tokens,[idx])\n",
    "            yield doc\n",
    "            \n",
    "    elif content=='content':\n",
    "        cleaner=CleanStringSpace()\n",
    "        tokenizer=TokenizeDocs()\n",
    "        count=0\n",
    "        for text in file_list:\n",
    "        \n",
    "            text=cleaner.transform(file_paths=[text],file=False)[0]\n",
    "            tokens=tokenizer.transform([text])[0]\n",
    "            doc=gensim.models.doc2vec.TaggedDocument(tokens,[count])\n",
    "            count+=1\n",
    "            yield doc\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorizeDoc(object):\n",
    "        \n",
    "    def __init__(self,doc_model):\n",
    "        self.doc_model=doc_model\n",
    "        \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,tokens,y=None):\n",
    "        \n",
    "        first=True\n",
    "        X=None\n",
    "        \n",
    "        for row in tokens:\n",
    "            vector=self.doc_model.infer_vector(np.array(row))\n",
    "            if first:\n",
    "                first=False\n",
    "                X=np.c_[np.reshape(vector,(1,-1))]\n",
    "            else:\n",
    "                X=np.vstack((X,np.c_[np.reshape(vector,(1,-1))]))\n",
    "\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self,tokens,y=None):\n",
    "        return self.transform(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"train/train.csv\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-22 15:23:46,793 : INFO : collecting all words and their counts\n",
      "2018-01-22 15:23:46,795 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-01-22 15:23:48,555 : INFO : PROGRESS: at example #10000, processed 689893 words (392565/s), 40037 word types, 10000 tags\n",
      "2018-01-22 15:23:50,298 : INFO : PROGRESS: at example #20000, processed 1367442 words (388870/s), 61467 word types, 20000 tags\n",
      "2018-01-22 15:23:51,990 : INFO : PROGRESS: at example #30000, processed 2032727 words (393438/s), 79072 word types, 30000 tags\n",
      "2018-01-22 15:23:53,757 : INFO : PROGRESS: at example #40000, processed 2722817 words (390872/s), 95257 word types, 40000 tags\n",
      "2018-01-22 15:23:55,505 : INFO : PROGRESS: at example #50000, processed 3392655 words (383512/s), 109552 word types, 50000 tags\n",
      "2018-01-22 15:23:57,354 : INFO : PROGRESS: at example #60000, processed 4095745 words (380487/s), 123158 word types, 60000 tags\n",
      "2018-01-22 15:23:59,094 : INFO : PROGRESS: at example #70000, processed 4777842 words (392131/s), 136180 word types, 70000 tags\n",
      "2018-01-22 15:24:00,856 : INFO : PROGRESS: at example #80000, processed 5458785 words (386540/s), 148506 word types, 80000 tags\n",
      "2018-01-22 15:24:02,550 : INFO : PROGRESS: at example #90000, processed 6121103 words (391366/s), 159782 word types, 90000 tags\n",
      "2018-01-22 15:24:04,281 : INFO : PROGRESS: at example #100000, processed 6794906 words (389482/s), 171013 word types, 100000 tags\n",
      "2018-01-22 15:24:06,080 : INFO : PROGRESS: at example #110000, processed 7488911 words (386075/s), 181873 word types, 110000 tags\n",
      "2018-01-22 15:24:07,749 : INFO : PROGRESS: at example #120000, processed 8146132 words (394022/s), 192408 word types, 120000 tags\n",
      "2018-01-22 15:24:09,474 : INFO : PROGRESS: at example #130000, processed 8819365 words (390581/s), 202513 word types, 130000 tags\n",
      "2018-01-22 15:24:11,275 : INFO : PROGRESS: at example #140000, processed 9501236 words (378845/s), 212481 word types, 140000 tags\n",
      "2018-01-22 15:24:13,048 : INFO : PROGRESS: at example #150000, processed 10184964 words (385723/s), 222366 word types, 150000 tags\n",
      "2018-01-22 15:24:14,709 : INFO : collected 231629 word types and 159571 unique tags from a corpus of 159571 examples and 10828458 words\n",
      "2018-01-22 15:24:14,710 : INFO : Loading a fresh vocabulary\n",
      "2018-01-22 15:24:14,863 : INFO : min_count=5 retains 44260 unique words (19% of original 231629, drops 187369)\n",
      "2018-01-22 15:24:14,864 : INFO : min_count=5 leaves 10558824 word corpus (97% of original 10828458, drops 269634)\n",
      "2018-01-22 15:24:15,011 : INFO : deleting the raw counts dictionary of 231629 items\n",
      "2018-01-22 15:24:15,019 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2018-01-22 15:24:15,021 : INFO : downsampling leaves estimated 7958769 word corpus (75.4% of prior 10558824)\n",
      "2018-01-22 15:24:15,022 : INFO : estimated required memory for 44260 words and 100 dimensions: 121366400 bytes\n",
      "2018-01-22 15:24:15,198 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "doc_model_corpus=gensim.models.doc2vec.Doc2Vec(size=100,window=5,min_count=5,alpha=0.025, min_alpha=0.025)\n",
    "doc_model_corpus.build_vocab(Tagged_Document_Gen(file_list=data['comment_text'].tolist(),content='content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-22 15:29:46,555 : INFO : training model with 3 workers on 44260 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-22 15:29:47,587 : INFO : PROGRESS: at 1.83% examples, 144473 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:29:48,620 : INFO : PROGRESS: at 4.18% examples, 168410 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:29:49,633 : INFO : PROGRESS: at 6.49% examples, 174905 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:50,663 : INFO : PROGRESS: at 8.79% examples, 178009 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:51,686 : INFO : PROGRESS: at 11.03% examples, 177426 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:52,706 : INFO : PROGRESS: at 13.41% examples, 178263 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:29:53,727 : INFO : PROGRESS: at 15.86% examples, 179877 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:54,778 : INFO : PROGRESS: at 18.27% examples, 180431 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:29:55,787 : INFO : PROGRESS: at 20.49% examples, 180864 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:56,840 : INFO : PROGRESS: at 22.90% examples, 181852 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:57,857 : INFO : PROGRESS: at 25.30% examples, 182579 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:58,890 : INFO : PROGRESS: at 27.75% examples, 183038 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:29:59,896 : INFO : PROGRESS: at 29.91% examples, 182030 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:00,907 : INFO : PROGRESS: at 31.94% examples, 180987 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:01,937 : INFO : PROGRESS: at 34.20% examples, 181401 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:02,949 : INFO : PROGRESS: at 36.50% examples, 182000 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:03,955 : INFO : PROGRESS: at 38.81% examples, 182587 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:04,988 : INFO : PROGRESS: at 41.04% examples, 182409 words/s, in_qsize 5, out_qsize 1\n",
      "2018-01-22 15:30:06,026 : INFO : PROGRESS: at 43.49% examples, 182563 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:07,076 : INFO : PROGRESS: at 45.89% examples, 182611 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:08,099 : INFO : PROGRESS: at 48.16% examples, 182540 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:09,104 : INFO : PROGRESS: at 50.39% examples, 182607 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:10,132 : INFO : PROGRESS: at 52.82% examples, 182799 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:11,144 : INFO : PROGRESS: at 55.27% examples, 183162 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:12,163 : INFO : PROGRESS: at 57.59% examples, 183150 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:13,205 : INFO : PROGRESS: at 59.96% examples, 183214 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:14,221 : INFO : PROGRESS: at 62.32% examples, 183217 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:15,242 : INFO : PROGRESS: at 64.66% examples, 183417 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:16,269 : INFO : PROGRESS: at 66.98% examples, 183554 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:17,277 : INFO : PROGRESS: at 69.20% examples, 183565 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:18,307 : INFO : PROGRESS: at 71.67% examples, 183702 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:19,327 : INFO : PROGRESS: at 73.94% examples, 183405 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:20,342 : INFO : PROGRESS: at 76.24% examples, 183379 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:21,365 : INFO : PROGRESS: at 78.57% examples, 183325 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:22,367 : INFO : PROGRESS: at 80.66% examples, 182929 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:23,391 : INFO : PROGRESS: at 82.87% examples, 182652 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:24,394 : INFO : PROGRESS: at 84.98% examples, 182310 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:25,407 : INFO : PROGRESS: at 87.19% examples, 182328 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:26,434 : INFO : PROGRESS: at 89.62% examples, 182473 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:27,463 : INFO : PROGRESS: at 91.98% examples, 182595 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:28,517 : INFO : PROGRESS: at 94.11% examples, 182218 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:29,540 : INFO : PROGRESS: at 96.41% examples, 182182 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:30,592 : INFO : PROGRESS: at 98.55% examples, 181721 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:30,973 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-22 15:30:30,999 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-22 15:30:31,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-22 15:30:31,004 : INFO : training on 10828458 raw words (8118254 effective words) took 44.4s, 182682 effective words/s\n",
      "2018-01-22 15:30:31,008 : INFO : saving Doc2Vec object under doc2vec_model_whole_corpus.model, separately None\n",
      "2018-01-22 15:30:31,013 : INFO : not storing attribute syn0norm\n",
      "2018-01-22 15:30:31,015 : INFO : storing np array 'doctag_syn0' to doc2vec_model_whole_corpus.model.docvecs.doctag_syn0.npy\n",
      "2018-01-22 15:30:31,230 : INFO : not storing attribute cum_table\n",
      "2018-01-22 15:30:31,897 : INFO : saved doc2vec_model_whole_corpus.model\n",
      "2018-01-22 15:30:31,924 : INFO : training model with 3 workers on 44260 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-01-22 15:30:32,980 : INFO : PROGRESS: at 1.83% examples, 141358 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:34,016 : INFO : PROGRESS: at 3.92% examples, 155632 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:35,027 : INFO : PROGRESS: at 5.97% examples, 159641 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:36,032 : INFO : PROGRESS: at 8.04% examples, 161776 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:37,045 : INFO : PROGRESS: at 10.31% examples, 166223 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:38,050 : INFO : PROGRESS: at 12.46% examples, 166868 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:39,059 : INFO : PROGRESS: at 14.75% examples, 168271 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:40,084 : INFO : PROGRESS: at 17.17% examples, 170830 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:41,150 : INFO : PROGRESS: at 19.55% examples, 172073 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:42,183 : INFO : PROGRESS: at 21.87% examples, 173596 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:43,199 : INFO : PROGRESS: at 23.94% examples, 173131 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:44,234 : INFO : PROGRESS: at 26.24% examples, 173673 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:45,239 : INFO : PROGRESS: at 28.63% examples, 174541 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:46,253 : INFO : PROGRESS: at 30.96% examples, 175609 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:47,297 : INFO : PROGRESS: at 33.26% examples, 176165 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:48,343 : INFO : PROGRESS: at 35.52% examples, 176708 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:49,354 : INFO : PROGRESS: at 37.74% examples, 177096 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:50,362 : INFO : PROGRESS: at 39.86% examples, 177117 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:51,406 : INFO : PROGRESS: at 42.24% examples, 177485 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:52,450 : INFO : PROGRESS: at 44.68% examples, 177865 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:53,476 : INFO : PROGRESS: at 46.66% examples, 176610 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:54,519 : INFO : PROGRESS: at 48.92% examples, 176978 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:55,536 : INFO : PROGRESS: at 51.18% examples, 176831 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:30:56,554 : INFO : PROGRESS: at 53.19% examples, 176183 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:57,571 : INFO : PROGRESS: at 55.63% examples, 176760 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:30:58,585 : INFO : PROGRESS: at 57.95% examples, 177010 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-22 15:30:59,616 : INFO : PROGRESS: at 60.34% examples, 177400 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:00,639 : INFO : PROGRESS: at 62.78% examples, 177828 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:01,662 : INFO : PROGRESS: at 65.11% examples, 178189 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:02,669 : INFO : PROGRESS: at 67.14% examples, 177897 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:03,676 : INFO : PROGRESS: at 69.38% examples, 178072 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:04,717 : INFO : PROGRESS: at 71.67% examples, 177864 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:05,719 : INFO : PROGRESS: at 74.11% examples, 178272 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:06,775 : INFO : PROGRESS: at 76.49% examples, 178407 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:07,786 : INFO : PROGRESS: at 78.94% examples, 178777 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:08,828 : INFO : PROGRESS: at 81.39% examples, 179123 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:09,846 : INFO : PROGRESS: at 83.81% examples, 179367 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:10,905 : INFO : PROGRESS: at 86.22% examples, 179645 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:11,912 : INFO : PROGRESS: at 88.50% examples, 179748 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-22 15:31:12,960 : INFO : PROGRESS: at 90.94% examples, 180027 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:13,970 : INFO : PROGRESS: at 93.29% examples, 180252 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:14,984 : INFO : PROGRESS: at 95.65% examples, 180482 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:16,009 : INFO : PROGRESS: at 98.10% examples, 180684 words/s, in_qsize 5, out_qsize 0\n",
      "2018-01-22 15:31:16,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-22 15:31:16,597 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-22 15:31:16,605 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-22 15:31:16,606 : INFO : training on 10828458 raw words (8117891 effective words) took 44.7s, 181729 effective words/s\n",
      "2018-01-22 15:31:16,608 : INFO : saving Doc2Vec object under doc2vec_model_whole_corpus.model, separately None\n",
      "2018-01-22 15:31:16,611 : INFO : not storing attribute syn0norm\n",
      "2018-01-22 15:31:16,614 : INFO : storing np array 'doctag_syn0' to doc2vec_model_whole_corpus.model.docvecs.doctag_syn0.npy\n",
      "2018-01-22 15:31:16,944 : INFO : not storing attribute cum_table\n",
      "2018-01-22 15:31:17,606 : INFO : saved doc2vec_model_whole_corpus.model\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    doc_model_corpus.train(Tagged_Document_Gen(file_list=data['comment_text'].tolist(),content='content'),total_examples=doc_model_corpus.corpus_count,epochs=1)\n",
    "    doc_model_corpus.save(\"doc2vec_model_whole_corpus.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-22 15:31:28,483 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('camera', 0.6153793931007385),\n",
       " ('battery', 0.5974810123443604),\n",
       " ('store', 0.5901130437850952),\n",
       " ('cars', 0.5810708999633789),\n",
       " ('terminal', 0.5806562900543213),\n",
       " ('shop', 0.5793058276176453),\n",
       " ('plane', 0.5783781409263611),\n",
       " ('studio', 0.5726466178894043),\n",
       " ('cd', 0.5711696147918701),\n",
       " ('pilot', 0.5681911706924438)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_model_corpus.wv.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(steps=[('clean_string',CleanStringSpace()),\n",
    "                       ('tokenize',TokenizeDocs()),\n",
    "                       ('vectorize',VectorizeDoc(doc_model_corpus)),\n",
    "                        ('scale',StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(np.array(data['comment_text']),np.array(data[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]),test_size=0.2)\n",
    "\n",
    "\n",
    "X_train=pipeline.fit_transform(X_train)\n",
    "X_test=pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 100)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(X_train,open(\"X_train.p\",\"wb\"))\n",
    "pickle.dump(X_test,open(\"X_test.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=Sequential()\n",
    "nn.add(Dense(64,activation='relu',input_shape=(100,)))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(32,activation='relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(16,activation='relu'))\n",
    "nn.add(BatchNormalization())\n",
    "nn.add(Dense(6,activation='sigmoid'))\n",
    "optimizer=Adam()\n",
    "nn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "127656/127656 [==============================] - 11s 90us/step - loss: 0.1395 - acc: 0.9524\n",
      "Epoch 2/10\n",
      "127656/127656 [==============================] - 11s 82us/step - loss: 0.0975 - acc: 0.9686\n",
      "Epoch 3/10\n",
      "127656/127656 [==============================] - 10s 81us/step - loss: 0.0948 - acc: 0.9693\n",
      "Epoch 4/10\n",
      "127656/127656 [==============================] - 10s 79us/step - loss: 0.0927 - acc: 0.9698\n",
      "Epoch 5/10\n",
      "127656/127656 [==============================] - 10s 78us/step - loss: 0.0911 - acc: 0.9700\n",
      "Epoch 6/10\n",
      "127656/127656 [==============================] - 10s 78us/step - loss: 0.0900 - acc: 0.9702\n",
      "Epoch 7/10\n",
      "127656/127656 [==============================] - 10s 79us/step - loss: 0.0895 - acc: 0.9703\n",
      "Epoch 8/10\n",
      "127656/127656 [==============================] - 10s 79us/step - loss: 0.0886 - acc: 0.9706\n",
      "Epoch 9/10\n",
      "127656/127656 [==============================] - 10s 80us/step - loss: 0.0876 - acc: 0.9706\n",
      "Epoch 10/10\n",
      "127656/127656 [==============================] - 11s 85us/step - loss: 0.0868 - acc: 0.9707\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f528907400>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train,y_train,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.35      0.49     12246\n",
      "          1       0.38      0.09      0.14      1275\n",
      "          2       0.81      0.31      0.45      6742\n",
      "          3       0.08      0.02      0.03       381\n",
      "          4       0.72      0.25      0.37      6269\n",
      "          5       0.26      0.05      0.09      1120\n",
      "\n",
      "avg / total       0.75      0.29      0.42     28033\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred=nn.predict(X_train)\n",
    "print(classification_report(y_train,(y_pred>0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string_with_space(s):\n",
    "    s = s.replace(u'\\n', u' ')\n",
    "    s = s.replace(u'’', u' ')\n",
    "    s = s.replace(u\"'\", u' ')\n",
    "    s = ''.join((c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c) != 'Mn'))\n",
    "    s = s.replace('.', ' ')\n",
    "    s = ''.join(e for e in s if (e.isalnum() or e == \" \")).lower()\n",
    "    while '  ' in s:\n",
    "        s = s.replace('  ', ' ')\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['comment_text_clean']=data['comment_text'].apply(lambda row:clean_string_with_space(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(data,open(\"data.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(input='content',min_df=5,max_features=20000)\n",
    "#vectorizer=CountVectorizer(input='content',min_df=1,max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(np.array(data['comment_text_clean']),np.array(data[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]),test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectorizer.fit_transform(X_train)\n",
    "scaler=StandardScaler(with_mean=False)\n",
    "X_train=scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X_train))\n",
    "X_train=X_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=Sequential()\n",
    "nn.add(Dense(16,activation='relu',input_shape=(20000,)))\n",
    "nn.add(Dense(6,activation='sigmoid'))\n",
    "optimizer=Adam()\n",
    "nn.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(X_train,y_train,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=nn.predict(X_train)\n",
    "print(classification_report(y_train,(y_pred>0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=vectorizer.transform(X_test)\n",
    "X_test=scaler.transform(X_test)\n",
    "X_test=X_test.todense()\n",
    "y_pred=nn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,(y_pred>0.5).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test=pd.read_csv(\"test/test.csv\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test['comment_text_clean']=data_test['comment_text'].apply(lambda row:clean_string_with_space(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=vectorizer.transform(np.array(data_test['comment_text_clean']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=svm.predict(X_test\n",
    "\n",
    "print(classification_report(np.array(data_test[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_1=log_loss(np.array(data_test[['toxic']]),y_pred[:,0])\n",
    "loss_2=log_loss(np.array(data_test[['severe_toxic']]),y_pred[:,1])\n",
    "loss_3=log_loss(np.array(data_test[['obscene']]),y_pred[:,2])\n",
    "loss_4=log_loss(np.array(data_test[['threat']]),y_pred[:,3])\n",
    "loss_5=log_loss(np.array(data_test[['insult']]),y_pred[:,4])\n",
    "loss_6=log_loss(np.array(data_test[['identity_hate']]),y_pred[:,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.average([loss_1,loss_2,loss_3,loss_4,loss_5,loss_6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
